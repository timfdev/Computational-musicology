---
title: "An analysis on Mac Millers' music"
author: "Tim Frohlich"
date: "March 2021"
output: 
  flexdashboard::flex_dashboard:
    css: portfolio.css
    storyboard: true
    theme: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(spotifyr)
library(usethis)
library(flexdashboard)
library(plotly)
library(compmus)
library(tidymodels)
library(ggdendro)
library(heatmaply)
```

<script src="../../R/x86_64-pc-linux-gnu-library/3.4/crosstalk/lib/jquery/jquery.js"></script>

### Introduction {data-commentary-width=400}

<div id="text">
<center>
<h1>Mac Miller</h1>
<h4>[1992-2018]</h4>
</center>
**The corpus**\
The corpus for this portofolio will be containing several albums of Mac Miller which results in a total of 122 tracks over the time period 2010-2020. Each album is collected from public spotify playlists. What is interesting about this corpus is that his music evolved a lot from his first album to his last. Over time he opens up more and his music changes with him. At the start of his career I feel like he created happy and typical party rap tracks, over time this changes to more serious topics like depression(his last album before his death). At some point he also starts singing more and produces more jazz-like tracks.

I will analyze the timeline of his musical career and observe how it progressed over the years. Mac miller was struggling with mental issues leading up to his death. Does this show in his music? I will be looking mainly at valence, energy and danceability to classify his albums emotionally and observe how his music changed.

**Natural comparison points**\
I expect to find a difference in valence and danceability between Mac Millers' first albums and later albums. Perhaps a difference in energy as well.

**Weaknesses of the corpus**\
Mac Miller has made a lot of music, quite a few of them mixtapes and not all of them are in a playlist on spotify. All his albums are on spotify so I chose to include all of his albums and two chosen mixtapes, which I feel are significant and were available on spotify. I felt these were significant because they are quite recent and have had positive critic reviews.


**Typical tracks**\
A few typical tracks out of the corpus: Album Swimming (Addiction and depression, sad tone) 2018. These examples are tracks which I personally chose as outlier tracks, either happy or sad. They are however not picked by spotify as typical outliers when looking at valence, energy and danceability.
<ul>- Self care (He raps about losing his mind here)</ul>

Album the divine Feminine(Love, happy/relaxed tone with more singing) 2016
<ul>
- We
</ul>

Mixtape Macadelic (Album after his flop, Blue slide park, optimistic/hopeful tone) 2012
<ul>- Thoughts from a balcony</ul>

Album Kids (Smoking and partying, relaxed/happy tone) 2010
<ul>
- Nikes on my feet
- Kool aid and frozen pizza test
</ul>
</div>

***

<div id="slideshow">
  <div>
  <img src="macmiller.gif" id="mac">
  </div>
  <div>
  <img src="macmiller2.gif" id="mac">
  </div>
  <div>
  <img src="macmiller3.gif" id="mac">
  </div>
  <div>
  <img src="macmiller4.jpg" id="mac">
  </div>
</div>

```{js echo=FALSE}
$("#slideshow > div").hide();
$(document).ready(function(){
  $("#slideshow > div:first").fadeIn(1000);
  $('#text').slideDown(2000);
});

$("#slideshow > div:gt(0)").hide();
setInterval(function() {
$('#slideshow > div:first')
.fadeOut(2000)
.next()
.fadeIn(2000)
.end()
.appendTo('#slideshow');
}, 4000);
};

```

### Combining features to measure general happiness on Mac Millers' albums.

```{r Plot, echo=FALSE}
  #Swimming
  Album1 <- get_playlist_audio_features("", "0LGaHnLkACbfrYgJ3XhRHV")
  #Macadelic
  Album2 <-get_playlist_audio_features("","2mJAUGPJcHA1sECMk76t7E")
  #KIDS
  Album3 <-get_playlist_audio_features("","3pfk7ZDZl6tfM2qsRGVMwR")
  #BlueSlidePark
  Album4 <-get_playlist_audio_features("","60IqtQHpL2eUT3IBogE1P5")
  #WatchingMoviesWithTheSoundOff
  Album5 <-get_playlist_audio_features("","74i9nnlnlSbBhVHtd5QDdI")
  #GO:ODAM
  Album6 <-get_playlist_audio_features("","3g8zMdBWNb3UdqfKbwyk3l")
  #DivineFeminime
  Album7 <-get_playlist_audio_features("","1pY5IqIUpYKjXz2VsHG00u")
  #Circles
  Album8 <-get_playlist_audio_features("","3TNIaVgNKhkIXWOyze0fvf")
```


```{r echo=FALSE}
all_albums <- Album1 %>%
  mutate(album_name="Swimming") %>%
  bind_rows(Album2 %>% mutate(album_name="Macadelic")) %>%
  bind_rows(Album3 %>% mutate(album_name="KIDS")) %>%
  bind_rows(Album4 %>% mutate(album_name="BlueSlidePark")) %>%
  bind_rows(Album5 %>% mutate(album_name="WatchingMovies\nWithTheSoundOff")) %>%
  bind_rows(Album6 %>% mutate(album_name="GO:ODAM")) %>%
  bind_rows(Album7 %>% mutate(album_name="DivineFeminime")) %>%
  bind_rows(Album8 %>% mutate(album_name="Circles")) %>%
  mutate(album_name = fct_relevel(album_name, "KIDS", "BlueSlidePark",
                           "Macadelic", "WatchingMovies\nWithTheSoundOff",
                           "GO:ODAM", "DivineFeminime", "Swimming", "Circles"))
```

```{r echo=FALSE, fig.width}
feature_distribution <- ggplot(all_albums,
    aes(
      x=danceability,
      y=valence,
      color=album_name,
      size=energy,
      label=track.name,
      alpha=0.8
      )
    ) +
  geom_point() +
  labs(title="Valence, energy and danceability per album",
       x="Danceability",
       y= "Valence"
       ) +
  facet_wrap(~album_name, scales="free_x") +
  scale_x_continuous(
    limits = c(0,1),
    breaks = c(0.0,0.25,0.50,0.75,1)
  ) +
  scale_y_continuous(
    limits = c(0,1),
    breaks = c(0.0,0.25,0.50,0.75,1)
  ) +
  scale_size_continuous(
    range=c(1,6)
  ) +
  theme_minimal() +
  theme(legend.position = "none")
ggplotly(feature_distribution)
```
***

<div id="commentary">
To analyze how happy a track is we look at the valence, energy and danceability per album.

> Valence is plotted against danceability with energy mapped to size.

The albums are sorted in chronological order from left to right so we easily analyse the results in relation to the timeline of Mac Millers' music.
As expected there is a clear difference between his first and his later albums in every category. **On average his first albums are happier, dancier and have more energy than his later albums.**
</div>

*(For some reason ggplotly removes the axis labels. You can see them if minimize your window.)*

### Happiness measured with just valence.

```{r echo=FALSE}
feature_distribution <- ggplot(all_albums,
    aes(
      x=album_name,
      y=valence,
      color=album_name,
      alpha=0.8
      )
    ) +
  geom_boxplot() +
  labs(title="The valence distribution per album",
       x="Album",
       y= "Valence"
       ) +
  theme_minimal() +
  theme(legend.position = "none")
ggplotly(feature_distribution)
```

```{r echo=FALSE}
# max_val <- all_albums %>%
#   top_n(2, valence)
# min_val = all_albums[which.min(all_albums$valence),]
# min_val$track.name
# min_val$album_name

```

***

<div id="commentary">
According to spotify, the valence feature is a measure of happiness. Therefore, I will look solely at the average valence per album to get a deeper analysis on happiness over the course of Mac Millers' career.
<br>
<br>
In general it is clear that <b>the average valence per album is dropping from Macs first album to his last.</b>
<br>
However it seems there <b>one</b> album that is an <b>exception</b> to this conclusion.
<br>
<ul>
- The Divine Feminime
</ul>
<br>
This exception can be explained by taking a closer look at this specific album. On this album <b>Mac Miller collaberated with his girlfriend</b> (Ariana Grande) and thus it makes sense that this album is a happy exception in a timeline that is dropping in valence steadily.
<br>
<br>
Additionally, the happiest and the saddest song of this corpus are selected as well for further analysis.
Track with the highest valence:
<ul>
 - 'Clubhouse'
</ul>

Track with the lowest valence:
<ul>
 - 'I can see'
</ul>
</div>

### Does pitch have influence on the classification of a song being happy for this corpus?

```{r echo=FALSE}
max_track <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

min_track <-
  get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

bind_rows(
  max_track %>%
    mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
    mutate(name = "Clubhouse") %>%
    compmus_gather_chroma(),
  min_track %>%
    mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
    mutate(name = "I can see") %>%
    compmus_gather_chroma()
) %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  xlim(0,160) +
  geom_tile() +
  facet_wrap(~name) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

```

***
<div id="text">
To see how pitch relates to the classification of a song being happy or sad, we compare the chromagram of the happiest track in this corpus with the chromagram of the saddest track in this corpus.
<br>
<br>
As a reminder, Clubhouse is the happiest song. Having said that, it is surprising to me that it seems as it has less energy in its' pitch classes. I was expecting that happy tracks would have more energy in pitch than sad tracks. Maybe that is true in general but just not for these particular tracks.
<br>
<br>
When listening to these tracks, one explanation can be the difference of style in general for these tracks. In 'I can see' Mac Miller is at times singing more than rapping, which can explain the energy difference.
</div>

### Comparing timbre in the happiest and the saddest song
```{r echo=FALSE}

bzt <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bzt2 <-
  get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
  bind_rows(
  bzt %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Clubhouse"),
  bzt2 %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "I can see")
  ) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() +
  xlim(0,170) +
  ylim(0,170) +
  labs(x = "", y = "")
```

***
<div id="text">
Here we compare the track with the highest valence("clubhouse") with the track that has the lowest valence("I can see") with timbre based self-similarity matrix. In the first matrix we can see a distinct sound represented by the yellow edge on the plot. "Clubhouse" starts indeed with a buzzer sound and around the 70 second mark there is a scratch in the track which can be seen in the plot. The plot of "I can see" shows the chorus clearly because of its distinct sound. The chorus at 150 second mark is very clear for example.
<br>
</div>

### Chordogram on the happiest song in the corpus

```{r echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```


```{r echo=FALSE}
twenty_five <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
twenty_five %>%
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "manhattan",  # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***
<div id="text">
This chordogram is about Clubhouse, the happiest Mac Miller song according to spotify. In this song (and Mac Miller songs in general) there aren't any abrupt key changes which is usually the case in rap, so that's why the chordogram is very structured and you can't really draw any conclusions. One thing that does stand out is the intro sound, which kind of sounds like an alarm.
</div>

### Comparing High valence with low valence in tempograms

```{r echo=FALSE, out.width="50%"}
clubhouse <- get_tidy_audio_analysis("6PJasPKAzNLSOzxeAH33j2")

clubhouse %>%
  tempogram(window_size = 4, hop_size = 2, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title="The track with the highest valence: 'Clubhouse'") +
  theme_classic()

icansee <- get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ")

icansee %>%
  tempogram(window_size = 4, hop_size = 2, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title="The track with the lowest valence: 'I can see'") +
  theme_classic()
```

***
<div id="text">
Comparing the track with the highest valence("clubhouse") with the track that has the lowest valence("I can see"). You would expect that 'clubhouse' has a higher BPM than 'I can see'. We can see that this statement is true for these particular tracks. 'clubhouse' has a BPM of a little over 120 with spikes up to 160 BPM, while 'i can see' has a BPM of a little under 120 throughout the track. Which makes sense because spotify probably uses BPM calculations among other things to classify a tracks' valence.
</div>

### Does a classifier agree with the valence feature?
```{r echo=FALSE}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  
```

```{r echo=FALSE}
happy <- 
  bind_rows(Album3,Album7)
sad <-
  bind_rows(Album1,Album2, Album4, Album5, Album6, Album8)

mac <- bind_rows(happy %>% mutate(playlist = "happy") %>% slice_head(n = 20),
                 sad %>% mutate(playlist = "sad") %>% slice_head(n = 20)
                 )
```

```{r echo=FALSE}
mac_features <-
  mac %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))

mac_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = mac_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
  # step_range(all_predictors())    # Sets range to [0, 1].
```

```{r echo=FALSE}
mac_cv <- mac_features %>% vfold_cv(5)
```

```{r echo = FALSE}
knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
mac_knn <- 
  workflow() %>% 
  add_recipe(mac_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    mac_cv, 
    control = control_resamples(save_pred = TRUE)
  )
mac_knn %>% get_conf_mat() %>% autoplot(type = "heatmap") + labs(title= "KNN")
```

***
<div id="text">
Are previous assumptions about happiness correctly interpreted since they mainly focused on the valence feature?
<br>
If want to classify the albums above .5 in valence as happy and sad otherwise, we can see in the confusion matrix that the classifier is performing pretty well. Remember that the label 'happy' and the label 'sad' are based solely on valence. The classifier is learning on around 10 different features. Therefore we can conclude that assumptions based on valence are indeed correctly interpreted.
</div>

### Discussion

<div id="text">
<b>Conclusion</b>
<br>
After doing different musical analyses, I can conclude that Mac Millers' music gradually became sadder over the years. An important indicator is the valence feature in the Spotify api which the classifier shows to be one of the most important features when measuring happiness of a song. 
<br>
Furthermore, does his music change as in the sadder track Mac Miller is singing more which could be a factor in the valence measure.

<b>Reflection</b>
<br>
Although the corpus selection is interesting, it does lack comparison points for a good research question and thus a good analysis. To perform a good analysis, where all tabs are used to form a clear answer to the main research question, one would have to compare two different corpuses.
</div>

