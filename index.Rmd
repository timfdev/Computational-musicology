---
title: "Portofolio computational musicology"
author: "Tim Frohlich"
date: "2/15/2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(spotifyr)
library(usethis)
library(flexdashboard)
library(plotly)
library(compmus)
```

### Comparing High valence with low valence in tempograms

```{r echo=FALSE, out.width="50%"}
clubhouse <- get_tidy_audio_analysis("6PJasPKAzNLSOzxeAH33j2")

clubhouse %>%
  tempogram(window_size = 4, hop_size = 2, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title="The track with the highest valence: 'Clubhouse'") +
  theme_classic()
  
icansee <- get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ")

icansee %>%
  tempogram(window_size = 4, hop_size = 2, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title="The track with the lowest valence: 'I can see'") +
  theme_classic()
```

***

Comparing the track with the highest valence("clubhouse") with the track that has the lowest valence("I can see"). You would expect that 'clubhouse' has a higher BPM than 'I can see'. We can see that this statement is true for these particular tracks. 'clubhouse' has a BPM of a little over 120 with spikes up to 160 BPM, while 'i can see' has a BPM of a little under 120 throughout the track. Which makes sense because spotify probably uses BPM calculations among other things to classify a tracks' valence.

### Introduction

The corpus for this portofolio will be containing several albums of Mac Miller which results in a total of 122 tracks over the time period 2010-2020. Each album is collected from public spotify playlists. What is interesting about this corpus is that his music evolved a lot from his first album to his last. Over time he opens up more and his music changes with him. At the start of his career I feel like he created happy and typical party rap tracks, over time this changes to more serious topics like depression(his last album before his death). At some point he also starts singing more and produces more jazz-like tracks.

I will analyze the timeline of his musical career and observe how it progressed over the years. Mac miller was struggling with mental issues leading up to his death. Does this show in his music? I will be looking at valence, energy and danceability to classify his albums emotionally and observe how his music changed.

**Natural comparison points**\
I expect to find a difference in valence and danceability between Mac Millers' first albums and later albums. Perhaps a difference in energy as well.

**Weaknesses of the corpus**\
Mac Miller has made a lot of music, quite a few of them mixtapes and not all of them are in a playlist on spotify. All his albums are on spotify so I chose to include all of his albums and two chosen mixtapes, which I feel are significant and were available on spotify. I felt these were significant because they are quite recent and have had positive critic reviews.


**Typical tracks**\
A few typical tracks out of the corpus: Album Swimming (Addiction and depression, sad tone) 2018

    Self care (He raps about losing his mind here)
    Coming back to earth (Addiction)

Album the divine Feminine(Love, happy/relaxed tone with more singing) 2016

    We

Mixtape Macadelic (Album after his flop, Blue slide park, optimistic/hopeful tone) 2012

    Thoughts from a balcony

Album Kids (Smoking and partying, relaxed/happy tone) 2010

    Nikes on my feet
    Kool aid and frozen pizza test

### Visualisation of valence,danceability and energy per album
```{r Plot, echo=FALSE}
  #Swimming
  Album1 <- get_playlist_audio_features("", "0LGaHnLkACbfrYgJ3XhRHV")
  #Macadelic
  Album2 <-get_playlist_audio_features("","2mJAUGPJcHA1sECMk76t7E")
  #KIDS
  Album3 <-get_playlist_audio_features("","3pfk7ZDZl6tfM2qsRGVMwR")
  #BlueSlidePark
  Album4 <-get_playlist_audio_features("","60IqtQHpL2eUT3IBogE1P5")
  #WatchingMoviesWithTheSoundOff
  Album5 <-get_playlist_audio_features("","74i9nnlnlSbBhVHtd5QDdI")
  #GO:ODAM
  Album6 <-get_playlist_audio_features("","3g8zMdBWNb3UdqfKbwyk3l")
  #DivineFeminime
  Album7 <-get_playlist_audio_features("","1pY5IqIUpYKjXz2VsHG00u")
  #Circles
  Album8 <-get_playlist_audio_features("","3TNIaVgNKhkIXWOyze0fvf")
```


```{r echo=FALSE}
all_albums <- Album1 %>%
  mutate(album_name="Swimming") %>%
  bind_rows(Album2 %>% mutate(album_name="Macadelic")) %>%
  bind_rows(Album3 %>% mutate(album_name="KIDS")) %>%
  bind_rows(Album4 %>% mutate(album_name="BlueSlidePark")) %>%
  bind_rows(Album5 %>% mutate(album_name="WatchingMovies\nWithTheSoundOff")) %>%
  bind_rows(Album6 %>% mutate(album_name="GO:ODAM")) %>%
  bind_rows(Album7 %>% mutate(album_name="DivineFeminime")) %>%
  bind_rows(Album8 %>% mutate(album_name="Circles")) %>%
  mutate(album_name = fct_relevel(album_name, "KIDS", "BlueSlidePark",
                           "Macadelic", "WatchingMovies\nWithTheSoundOff",
                           "GO:ODAM", "DivineFeminime", "Swimming", "Circles"))
```
 
```{r echo=FALSE, fig.width} 
feature_distribution <- ggplot(all_albums, 
    aes(
      x=danceability, 
      y=valence, 
      color=album_name,
      size=energy, 
      label=track.name,
      alpha=0.8
      )
    ) +
  geom_point() +
  labs(title="The valence plotted against the danceability with energy mapped to size",
       x="Danceability", 
       y= "Valence"
       ) +
  facet_wrap(~album_name, scales="free_x") +
  scale_x_continuous(
    limits = c(0,1),
    breaks = c(0.0,0.25,0.50,0.75,1)
  ) +
  scale_y_continuous(
    limits = c(0,1),
    breaks = c(0.0,0.25,0.50,0.75,1)
  ) +
  scale_size_continuous(
    range=c(1,6)
  ) +
  theme_minimal() +
  theme(legend.position = "none")
ggplotly(feature_distribution)
```
***

In this graph we plot the valence against the danceability in an interactive plot with the energy of a track mapped to size. The albums are in chronological order from left to right. We can see in the plots that the style in albums differs a lot. Mac Millers' first albums are classified as more happy and danceable by spotify.

*(For some reason ggplotly removes the axis labels. You can see them if minimize your window. The same bug is on dr Burgoyne's portofolio.)*

### Visualisation of valence per album
```{r echo=FALSE}
feature_distribution <- ggplot(all_albums, 
    aes(
      x=album_name, 
      y=valence, 
      color=album_name,
      alpha=0.8
      )
    ) +
  geom_boxplot() +
  labs(title="The valence distribution per album",
       x="Album", 
       y= "Valence"
       ) +
  theme_minimal() +
  theme(legend.position = "none")
ggplotly(feature_distribution)
```

```{r echo=FALSE}
max_val <- all_albums %>%
  top_n(2, valence)
min_val = all_albums[which.min(all_albums$valence),]
min_val$track.name
min_val$album_name
```

***

According to spotify, the valence feature is a measure of happiness. Therefore, I will zoom in on the average valence per album. We can see that from the first album the average valence is dropping until the album: "The Divine Feminime". This makes sense because he made this album with Ariana Grande, his girlfriend at the time. In this relationship he was visually in a good mental state, after their break up Mac Miller struggled with mental issues and this does show in his albums after "The Divine Feminime.".\
Furthermore, the track with the highest valence is:\
- "`r max_val$track.name[2]`" from "`r max_val$album_name[2]`".\

The track with the lowest valence is:\
- "`r min_val$track.name`" from "`r min_val$album_name`".\
In the next visualization page, these tracks will be further analyzed.

### Zooming in on a track

```{r echo=FALSE}
max_track <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
max_track %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chroma of the song 'Clubhouse' from the album 'GO:ODAM'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

```

***
The track with the highest valence is actually "Mad Flava, Heavy Flow" fom the album "KIDS" but since this track is merely an interlude, the track with the second highest valence is observed in a chroma.

### Zooming in on a track 2

```{r echo=FALSE}
min_track <-
  get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
min_track %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chroma of the song  'I can see' from the album 'Circles'") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

```

### Investigating structure with the chroma feature
```{r echo=FALSE}
#Clubhouse, Goodam, 7rdyAfIm1t6h6I1gyLtD17
#I can see, Circles, 3R8CyhJfVjvgIROd5RSGhQ
bzt <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) 
bzt2 <-
  get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) 
  bind_rows(
  bzt %>% 
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "Clubhouse"), 
  bzt2 %>%
    compmus_self_similarity(pitches, "aitchison") %>% 
    mutate(d = d / max(d), type = "I can see")
  ) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***
Here we compare the track with the highest valence("clubhouse") with the track that has the lowest valence("I can see") with chroma based self-similarity matrices. The matrices show that both tracks stay relatively the same throughout its' entire duration. Both tracks have a clear structure that is predictable. "Clubhouse" can be divided in three parts each taking around 60 seconds but does not have real clear pitch changes compared to "I can see". "I can see" has very clear pitch changes which can be seen in the chroma matrix from 0 until 50 seconds for example.

### Investigating structure with the timbre feature
```{r echo=FALSE}
#Clubhouse, Goodam, 7rdyAfIm1t6h6I1gyLtD17
#I can see, Circles, 3R8CyhJfVjvgIROd5RSGhQ
bzt <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bzt2 <-
  get_tidy_audio_analysis("3R8CyhJfVjvgIROd5RSGhQ") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
  bind_rows(
  bzt %>% 
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "Clubhouse"), 
  bzt2 %>%
    compmus_self_similarity(timbre, "euclidean") %>% 
    mutate(d = d / max(d), type = "I can see")
  ) %>%
  mutate() %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

***
Here we compare the track with the highest valence("clubhouse") with the track that has the lowest valence("I can see") with timbre based self-similarity matrix. In the first matrix we can see a distinct sound represented by the yellow edge on the plot. "Clubhouse" starts indeed with a buzzer sound and around the 70 second mark there is a scratch in the track which can be seen in the plot. The plot of "I can see" shows the chorus clearly because of its distinct sound. The chorus at 150 second mark is very clear for example.

### Chordogram on the happiest song in the corpus 

```{r echo=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```


```{r echo=FALSE}
twenty_five <-
  get_tidy_audio_analysis("7rdyAfIm1t6h6I1gyLtD17") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
twenty_five %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "manhattan",  # Try different distance metrics
    norm = "euclidean"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

This chordogram is about Clubhouse, the happiest Mac Miller song according to spotify. In this song (and Mac Miller songs in general) there aren't any abrupt key changes which is usually the case in rap, so that's why the chordogram is very structured and you can't really draw any conclusions. One thing that does stand out is the intro sound, which kind of sounds like an alarm.

### Discussion
This analysis shows the mental rollercoaster of Mac Miller, it is not completely unbiased however. It does include all his albums and two mixtapes but it does not include all his music over his entire career. The reason for this is that not all his mixtapes are available on spotify. We can certainly conclude that his last two albums  leading up to his death were of a more depressed tone.

